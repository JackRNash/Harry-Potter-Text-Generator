{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from scipy import spatial\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments in Text Generation\n",
    "In preparation for a more advanced project, I wanted to play around with natural text generation. I decided to try and generate text in the style of Harry Potter. Inspired by [Karpathy's post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), I first tried things with an LSTM that generates text character by character. I highly reccomend reading his post if you are unfamiliar with recurrent neural networks. A one sentence summary is they taken a sequence of timesteps(in this case characters) and produce a sequence of outputs based on their hidden state which stores information about previous timesteps. Long Short-Term Memory(LSTMs) are a more complicated structure that does a similar thing but avoids problems like vanishing gradients and helps keep things in memory for longer.\n",
    "\n",
    "I did not include the texts in the repo because I don't want to distribute copyrighted materials..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the texts (only have first three because getting others was a little troublesome)\n",
    "text_file1 = open('texts/J. K. Rowling - Harry Potter 1 - Sorcerer\\'s Stone.txt', 'r')\n",
    "text_file2 = open('texts/J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt', 'r')\n",
    "text_file3 = open('texts/J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt', 'r')\n",
    "#text_file4 = open('texts/J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt', 'r')\n",
    "\n",
    "book1 = text_file1.read()\n",
    "book2 = text_file2.read()\n",
    "book3 = text_file3.read()\n",
    "#book4 = text_file4.read()\n",
    "\n",
    "books = ''.join((book1, book2, book3))\n",
    "chars = list(set(books))\n",
    "#int2char = {i:c for i, c in enumerate(chars)}\n",
    "char2int = {c:i for i, c in enumerate(chars)}\n",
    "\n",
    "encoded_books = np.array([char2int[c] for c in books])\n",
    "\n",
    "def one_hot_enc(arr, vocab_size):\n",
    "    '''\n",
    "    Inputs:\n",
    "    arr - array containing message to be encoded\n",
    "    vocab_size - number of unique chars\n",
    "    \n",
    "    Output: \n",
    "    one_hot - encoded array of shape (arr.shape, vocab_size) i.e. shape of arr with extra dimension appended\n",
    "    '''\n",
    "    # Unroll the message array as rows and expand vocab_size columns to form the shape for one_hot\n",
    "    if(len(arr.shape) > 1): # multiple dims(batch size > 1)\n",
    "        size = np.multiply(*arr.shape)\n",
    "    else:\n",
    "        size = arr.shape[0]\n",
    "    one_hot = np.zeros((size, vocab_size), dtype=np.float32) \n",
    "    \n",
    "    # Look at row i(for all i), make one_hot[i, arr.flatten()[i]] = 1\n",
    "    one_hot[np.arange(size), arr.flatten()] = 1 \n",
    "    \n",
    "    # Fix the shape\n",
    "    one_hot = one_hot.reshape(*arr.shape, vocab_size)\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def batch_generator(data, batch_size, seq_len):\n",
    "    '''\n",
    "    data - shape (total_chars,)\n",
    "    '''\n",
    "\n",
    "    chars_per_batch = batch_size * seq_len\n",
    "    batches = len(data) // chars_per_batch\n",
    "    data = data[:batches*chars_per_batch] # truncate extra chars that wouldn't make a complete batch\n",
    "    data = np.reshape(data, (batch_size, -1))\n",
    "\n",
    "    # Yield a sliding window of data of size batch_size*seq_len as well as a window slid one to the right\n",
    "    # The offset window is used to evaluate whether the model accurately predicted the next char\n",
    "    for i in range(0, data.shape[1], seq_len):\n",
    "        x = np.zeros((batch_size, seq_len))\n",
    "        x_offset = np.zeros((batch_size, seq_len))\n",
    "        \n",
    "        # Our input\n",
    "        x = data[:, i:i+seq_len]\n",
    "        \n",
    "        # Make our off by one target\n",
    "        temp = data.flatten()\n",
    "        temp = np.roll(temp, shift=-1)\n",
    "        temp = np.reshape(temp, data.shape)\n",
    "        x_offset = temp[:, i:i+seq_len]\n",
    "    \n",
    "        yield x, x_offset\n",
    "        \n",
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, chars, n_layers, hidden_dim, dropout=0.3):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        # Important numbers to save\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = len(chars)\n",
    "        \n",
    "        # Create conversion dictionaries\n",
    "        self.int2char = {i:c for i, c in enumerate(chars)}\n",
    "        self.char2int = {c:i for i, c in enumerate(chars)}\n",
    "        \n",
    "        # Actual layers of model\n",
    "        self.lstm = nn.LSTM(input_size=self.vocab_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, \n",
    "                            dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, self.vocab_size)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=device),\n",
    "               torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "An important part of any text generation network is the sampling. Even if you develop a model that outputs a very useful/realistic probability distribution of the next char/word, if you pick from that distribution poorly you can still have bad results. I chose to use nucleus sampling/p-sampling given it's remarkable results in other models. The following few functions will be useful later for our fancier models. For now, only pay attention to the code where `is_char_model` is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample(x, p=.9):\n",
    "    '''\n",
    "    p sampling, aka nucleus sampling, looks at the top x percent of most likely next words/characters and samples from that\n",
    "    \n",
    "    x - tensor with x[:, -1] containing the probability distribution for entire batch\n",
    "    p - min probability for mass (e.g. if p=.9, take the top 90% most likely words)  0 <= p <= 1\n",
    "    '''\n",
    "    sorted_vals, sorted_indices = torch.sort(x, dim=-1, descending=True)\n",
    "\n",
    "    cum_probs = torch.cumsum(sorted_vals, dim=-1)\n",
    "    indices_too_small = cum_probs > p\n",
    "    \n",
    "    # Shift by one to make sure first token over the threshold is also included\n",
    "    indices_too_small = torch.roll(indices_too_small, shifts=1)\n",
    "    indices_too_small[0, 0]=0\n",
    "    \n",
    "    indices_too_small = sorted_indices[indices_too_small]  \n",
    "    x[:, indices_too_small] = -float('inf') # negative infinity so after softmax, no chance to be chosen\n",
    "    \n",
    "    return F.softmax(x, dim=-1)\n",
    "\n",
    "def predict_token(model, hc, input_token, p=.9, argmax=False, is_char_model=False):\n",
    "    '''\n",
    "    input_char - an int where int2token[input_token] is the desired token\n",
    "    hc - hidden states\n",
    "    p - min probability for mass (e.g. if p=.9, after sorting, take the top 90% most likely words)  0 <= p <= 1\n",
    "    argmax - if true, take the most likely token every time(doesn't produce great results)\n",
    "    char_model - True if model isinstance of CharRNN\n",
    "    '''  \n",
    "    input_token = np.array([input_token])\n",
    "    \n",
    "    if is_char_model:\n",
    "        one_hot = one_hot_enc(input_token, model.vocab_size)\n",
    "        model_input = torch.from_numpy(one_hot)\n",
    "    else:\n",
    "        model_input = torch.from_numpy(input_token)\n",
    "    \n",
    "    model_input = model_input.to(device)\n",
    "    model_input.to(torch.int64)\n",
    "    model_input = torch.unsqueeze(model_input, 0)\n",
    "    \n",
    "    if not is_char_model:\n",
    "        model_input = model_input.long()\n",
    "    x, hc = model(model_input, hc)\n",
    "\n",
    "    # Reshape so it's shape (1, vocab_size)\n",
    "    x = x.view(-1, model.vocab_size)\n",
    "    \n",
    "    if argmax:\n",
    "        probs = F.softmax(x, dim=-1)\n",
    "        token = torch.argmax(probs)\n",
    "        token_as_int = token_as_int.item()\n",
    "    else: # Create a probability distribution & sample\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        probs = p_sample(x, p=p)\n",
    "        prob_dist = torch.distributions.Categorical(probs)\n",
    "        token_as_int = prob_dist.sample().item()\n",
    "    \n",
    "    if is_char_model:  # since model input is words as ints\n",
    "        token = model.int2char[token_as_int]\n",
    "    else: # word model\n",
    "        token = int2word[token_as_int]\n",
    "    \n",
    "    return token, token_as_int, hc\n",
    "\n",
    "def predict_sequence_tokens(model, seed_phrase='Harry', length=500, p=.9, argmax=False, include_seed=False):\n",
    "    '''\n",
    "    Predict a string of specified length iteratively(i.e. predict each token based on prev. predicted tokens). Tokens \n",
    "    can be chars(CharRNN) or words (WordRNN)\n",
    "    \n",
    "    seed_phrase - a phrase to feed into the model to generate a specific initial hidden state\n",
    "    include_seed - if true, include the seed phrase as part of the output\n",
    "    length - length of generated string\n",
    "    argmax - if true, take the most likely token every time(doesn't produce great results)\n",
    "    p - min probability for mass (e.g. if p=.9, after sorting, take the top 90% most likely words)  0 <= p <= 1\n",
    "    '''\n",
    "    model.eval()\n",
    "    hc = model.init_hidden(1)\n",
    "    is_char_model = isinstance(model, CharRNN)\n",
    "    \n",
    "    input_token_num = 0 # to be input into the model after seeding, will be 0 if no input phrase\n",
    "    \n",
    "    if not is_char_model: # Create list and split out words from punctuation and whitespace\n",
    "        iter_phrase = re.split('([\\\"\\.,!\\?;:\\s]+)', seed_phrase) \n",
    "    else:\n",
    "        iter_phrase = seed_phrase\n",
    "    \n",
    "    for token in iter_phrase:\n",
    "        if not is_char_model:\n",
    "            num = word2int[token]\n",
    "        else:\n",
    "            num = model.char2int[token]\n",
    "        _token, input_token_num, hc = predict_token(model, hc, num, p=p, argmax=argmax, is_char_model=is_char_model)\n",
    "    \n",
    "    msg = []\n",
    "    if include_seed:\n",
    "        msg.append(seed_phrase)\n",
    "        msg.append(_token)\n",
    "        if is_char_model:\n",
    "            length -= len(seed_phrase)\n",
    "        else:\n",
    "            length -= 1\n",
    "    \n",
    "    for i in range(length):\n",
    "        word, input_token_num, hc = predict_token(model, hc, input_token_num, p=p, argmax=argmax, is_char_model=is_char_model)\n",
    "        msg.append(word)\n",
    "    \n",
    "    output_string = ''.join(msg) # don't join until end for efficiency\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, seq_len=100, batch_size=100, epochs=30, summary_every=30, print_time=False):\n",
    "    \n",
    "    if print_time:\n",
    "        prev_time = time.time()\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    iteration = 0\n",
    "    is_char_model = isinstance(model, CharRNN)\n",
    "    model.train()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Reset / initialize (hidden state, cell) params\n",
    "        hc = model.init_hidden(batch_size)\n",
    "        \n",
    "        rolling_loss = []\n",
    "        for x, x_offset in batch_generator(data, batch_size=batch_size, seq_len=seq_len):\n",
    "            iteration += 1\n",
    "            \n",
    "            if is_char_model: # only char model needs one hot encoding\n",
    "                x = one_hot_enc(x, model.vocab_size)\n",
    "            \n",
    "            # Copy over data in hc to prevent backprop through entire epoch(noticeably improves training time)\n",
    "            hc = [layer.data for layer in hc]\n",
    "            hc = tuple(hc)\n",
    "            \n",
    "            # Move to tensors and gpu if applicable\n",
    "            x = torch.from_numpy(x)\n",
    "            x = x.to(device)\n",
    "            x_offset = torch.from_numpy(x_offset)\n",
    "            x_offset = x_offset.to(device)\n",
    "            \n",
    "            # Clear gradients and predict\n",
    "            optimizer.zero_grad()\n",
    "            if not is_char_model: # Since x is words as integers, for word2vec embedding must explicitly change type\n",
    "                x = x.long()\n",
    "            prediction, hc = model(x, hc)\n",
    "            \n",
    "            # Reshape so that it looks like one giant batch of batch_size * seq_len predicting on vocab_size classes\n",
    "            # which is needed for the cross entropy loss function\n",
    "            prediction = prediction.view(-1, model.vocab_size)\n",
    "            x_offset = x_offset.contiguous().view(-1)\n",
    "                \n",
    "            # Get loss and take step\n",
    "            loss = criterion(prediction, x_offset.long())\n",
    "            loss.backward() # forgot this initially, oops!\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 10) # Avoid exploding gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            rolling_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "            if iteration % summary_every == 0:\n",
    "                if print_time:\n",
    "                    print('Epoch: {0} ... Iteration: {1} ... Current loss: {2:.6f} ... Average loss over last {3} \\\n",
    "                        iterations: {4:.6f} ... Time {3} iterations took: {5:.3f}'.format(\n",
    "                        e+1, iteration, loss.item(), summary_every, np.mean(rolling_loss), time.time() - prev_time))\n",
    "                    prev_time = time.time()\n",
    "                else:\n",
    "                    print('Epoch: {0} ... Iteration: {1} ... Current loss: {2:.6f} ... Average loss over last {3} \\\n",
    "                          iterations: {4:.6f}'.format(e+1, iteration, loss.item(), summary_every, np.mean(rolling_loss)))\n",
    "                print('-'*50)\n",
    "                print(predict_sequence_tokens(model, length=20))\n",
    "                print('\\n')\n",
    "                model.train()\n",
    "                rolling_loss = []              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 4\n",
    "hidden_dimension = 512\n",
    "dropout_chance = 0.3\n",
    "lr = 1e-3\n",
    "    \n",
    "char_net = CharRNN(chars, n_layers=num_layers, hidden_dim=hidden_dimension, dropout=dropout_chance)\n",
    "char_net = char_net.to(device)\n",
    "optim = torch.optim.Adam(char_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More hyperparameters (these just affect training)\n",
    "show_time = False\n",
    "summary_every = 5\n",
    "seq_len = 100\n",
    "batch_size = 10\n",
    "reset_optimizer = False\n",
    "lr = 1e-3 # won't be used unless reset optimizer is True\n",
    "\n",
    "if reset_optimizer:\n",
    "    optim = torch.optim.Adam(char_net.parameters(), lr=lr)\n",
    "train(char_net, encoded_books, optim, seq_len=seq_len, batch_size=batch_size, \n",
    "      summary_every=summary_every, print_time=show_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence of arbitrary length\n",
    "seed_phrase='Harry' # Initializes hidden state of network\n",
    "length=500 \n",
    "p=.9 # 0 <= p <=1  sample from the top p most likely characters\n",
    "greedy=False # choose most likely character every time(don't reccomend, easy for model to get off track and never recover)\n",
    "include_seed=False # Include seed in output\n",
    "\n",
    "sample = predict_sequence_tokens(char_net, seed_phrase=seed_phrase, length=length, p=p, argmax=greedy, include_seed=include_seed)\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Some of the short excerpts could get quite good.\n",
    "\n",
    "```Hagrid and Hermione shouted. ```\n",
    "\n",
    "```\"Ouch!\" said Madam Pomfrey suspiciously.```\n",
    "\n",
    "```\"But --\"```\n",
    "\n",
    "Others were close to making sense, but not quite there.\n",
    "\n",
    "```\"What do you think that was the boy?\" said Harry```\n",
    "\n",
    "```\"Why?\" Harry watched. \"What if he is\n",
    "today that the memory work,\" he whispered.```\n",
    "\n",
    "```\"Harry!\" Harry yelled. \"I could not mind me without you```\n",
    "\n",
    "Some were a little further off the mark, especially when the passages got longer.\n",
    "\n",
    "```\"It's out,\" he said white-hispinated and coldly,```\n",
    "\n",
    "```It had just thought Ron think we started to get in his eyes, for bed not\n",
    "a parchment like his back. Boys there was sure not five a grip at once,\n",
    "who had been safely with simple hand, Harry```\n",
    "\n",
    "Overall, considering the LSTM learned the entire English language character by character from a few Harry Potter books, these are pretty impressive results! It spelled words correctly quite often and picked up on some nuances of dialogue. Still, it leaves a lot to be desired which is why we'll investigate more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "The results of the character level LSTM were lackluster, although still impressive considering what it was learning from. I turned towards modeling the text a word level instead. This proved MUCH more complicated than I originally anticipated. I chose to use word2vec to embed my words. [word2vec](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) is a shallow neural network that learns to represent a vocabulary of m words in a dimension d where typically d << m. Research has found that there isn't much benefit in performance for representing the words with more that ~300 dimensions. \n",
    "\n",
    "Using a lower dimension helps speed up training by using less parameters and also because similar words are (hopefully) embedded with similar vectors which can help the model when picking the next word. For example, perhaps the ideal word would've been chair, but the model isn't quite accurate and gives stool, a similar word, a high probability of being the next word. Obviously this is much better than it being some random other word that had a vector close to chair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_loader(corpus, batch_size=10, seq_len=100, cutoff=3):\n",
    "    '''\n",
    "    corpus - string\n",
    "    cutoff - how many instances must appear\n",
    "    '''\n",
    "    # Preprocess the text\n",
    "    \n",
    "    # Split out words from punctuation and whitespace\n",
    "    word_list = re.split('([\\\"\\.,!\\?;:\\s]+)', corpus) \n",
    "    \n",
    "    # Create a dictionary that keeps track of how many times a word appears. Get rid of words that appear fewer\n",
    "    # than cutoff times\n",
    "    word_dict = {}\n",
    "    for word in word_list:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] += 1\n",
    "    cutoff_dict = {key:val for key,val in word_dict.items() if val > cutoff}\n",
    "    processed_corpus = [w for w in word_list if w in cutoff_dict]\n",
    "    vocab_size = len(cutoff_dict) + 1\n",
    "    \n",
    "    # Create our lookup dicts\n",
    "    word2idx = {word:idx for idx, word in enumerate(cutoff_dict.keys())}\n",
    "    idx2word = {idx:word for idx, word in enumerate(cutoff_dict.keys())}\n",
    "    idx2word[vocab_size - 1] = '_UNK_' # for words filtered out\n",
    "    word2idx['_UNK_'] = vocab_size - 1\n",
    "    \n",
    "    # Convert to ints\n",
    "    corpus_as_int = np.array([word2idx[w] for w in processed_corpus])\n",
    "    \n",
    "    # Prep for batches\n",
    "    words_per_batch = batch_size*seq_len\n",
    "    batches = len(corpus_as_int) // words_per_batch\n",
    "    data = corpus_as_int[:batches*words_per_batch] # truncate extra words that won't make a complete batch\n",
    "    data = np.reshape(data, (batch_size, -1))\n",
    "    \n",
    "    return [processed_corpus], word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging # show information for gensim while training/loading/saving\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "I was going to implement word2vec and then I found gensim which made it dead simple to train a word2vec model. Below is the code to train one yourself, or if you skip a head, you can import the model I used. It's not perfect, but it's decent enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "min_count = 3 # minimum amount of times a word must appear to be included\n",
    "books_ready, word2int, int2word = word_loader(books, cutoff=min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "prev_time = time.time()\n",
    "model = Word2Vec(books_ready, size=300, negative=20, iter=20, sample=1e-3, sg=1, min_count = min_count, workers=cpu_count())\n",
    "print('{:.5f} seconds'.format(time.time() - prev_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9821884369931323"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check how similar the vectors for 'Harry' and 'Potter' are. Looks good!\n",
    "model.similarity('Harry', 'Potter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-08-16 17:09:21,583 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spells', 0.6969023942947388),\n",
       " ('magical', 0.6892367601394653),\n",
       " ('sorcery', 0.6885683536529541),\n",
       " ('fiend', 0.6736955642700195),\n",
       " ('wizard', 0.6666735410690308),\n",
       " ('undead', 0.6593881249427795),\n",
       " ('stormbringer', 0.6543042659759521),\n",
       " ('diceless', 0.6521874666213989),\n",
       " ('hellboy', 0.6503678560256958),\n",
       " ('summoner', 0.6503649950027466)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vectors closest to 'magic's vector and how similar they are. Seems like for the most part\n",
    "# a reasonable list of words!\n",
    "model.most_similar('magic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-16 22:09:34,529 : INFO : saving Word2Vec object under model_w2v.model, separately None\n",
      "2019-08-16 22:09:34,530 : INFO : storing np array 'vectors' to model_w2v.model.wv.vectors.npy\n",
      "2019-08-16 22:09:34,852 : INFO : not storing attribute vectors_norm\n",
      "2019-08-16 22:09:34,854 : INFO : storing np array 'syn1neg' to model_w2v.model.trainables.syn1neg.npy\n",
      "2019-08-16 22:09:35,254 : INFO : not storing attribute cum_table\n",
      "2019-08-16 22:09:35,437 : INFO : saved model_w2v.model\n"
     ]
    }
   ],
   "source": [
    "model.save('w2v/model_w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Model\n",
    "Let's build our new model that can use our fancy `word2vec` embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-19 12:47:36,600 : INFO : loading Word2VecKeyedVectors object from w2v/model_w2v.model\n",
      "2019-08-19 12:47:36,766 : INFO : loading wv recursively from w2v/model_w2v.model.wv.* with mmap=None\n",
      "2019-08-19 12:47:36,767 : INFO : loading vectors from w2v/model_w2v.model.wv.vectors.npy with mmap=None\n",
      "2019-08-19 12:47:36,819 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-08-19 12:47:36,820 : INFO : loading vocabulary recursively from w2v/model_w2v.model.vocabulary.* with mmap=None\n",
      "2019-08-19 12:47:36,821 : INFO : loading trainables recursively from w2v/model_w2v.model.trainables.* with mmap=None\n",
      "2019-08-19 12:47:36,822 : INFO : loading syn1neg from w2v/model_w2v.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-08-19 12:47:36,875 : INFO : setting ignored attribute cum_table to None\n",
      "2019-08-19 12:47:36,876 : INFO : loaded w2v/model_w2v.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Use to load your saved w2v model instead of retraining\n",
    "model_w2v = KeyedVectors.load('w2v/model_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Need a way of keeping track which words correspond to which word2vec embeddings\n",
    "lookup = np.random.randn(len(word2int), 300)\n",
    "for i, w in enumerate(word2int.keys()):\n",
    "    try:\n",
    "        embed = model_w2v[w]\n",
    "        idx = word2int[w]\n",
    "        lookup[idx] = embed\n",
    "    except KeyError:\n",
    "        pass\n",
    "weights = torch.FloatTensor(lookup).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordRNN(nn.Module):\n",
    "    '''\n",
    "    A similar architecture to the character level RNN. This model predicts word by word instead of character by character\n",
    "    '''\n",
    "    def __init__(self, weights, vocab_size, n_layers=3, hidden_dim=512, dropout=.5):\n",
    "        super(WordRNN, self).__init__()\n",
    "        \n",
    "        # Important numbers to save\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Actual layers of model\n",
    "        self.embed = nn.Embedding.from_pretrained(weights)\n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True, \n",
    "                            dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, self.vocab_size)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        x = self.embed(x)\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=device),\n",
    "               torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 3\n",
    "hidden_dimension = 512\n",
    "dropout_chance = 0.3\n",
    "lr = 1e-3\n",
    "\n",
    "word_net = WordRNN(weights, vocab_size=len(word2int), n_layers=num_layers, hidden_dim=hidden_dimension, dropout=dropout_chance)\n",
    "word_net = word_net.to(device)\n",
    "optim = torch.optim.Adam(char_net.parameters(), lr=lr)\n",
    "encoded_books_as_words = np.array([word2int[word] for word in books_ready[0]]) # will be input as the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training\n",
    "show_time = False\n",
    "summary_every = 300\n",
    "seq_len = 100\n",
    "batch_size = 10\n",
    "reset_optimizer = False\n",
    "lr = 1e-3 # won't be used unless reset optimizer is True\n",
    "\n",
    "if reset_optimizer:\n",
    "    optim = torch.optim.Adam(char_net.parameters(), lr=lr)\n",
    "train(word_net, encoded_books_as_words, optim, epochs=20, seq_len=50, batch_size=10, summary_every=5, print_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence of arbitrary length\n",
    "seed_phrase='Harry' # Initializes hidden state of network(needs to be composed of words/punctuation/whitespace in vocab)\n",
    "length=500 \n",
    "p=.9 # 0 <= p <=1  sample from the top p most likely characters\n",
    "greedy=False # choose most likely character every time(don't reccomend, easy for model to get off track and never recover)\n",
    "include_seed=False # Include seed in output\n",
    "\n",
    "sample = predict_sequence_tokens(word_net, seed_phrase=seed_phrase, length=length, p=p, argmax=greedy, include_seed=include_seed)\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "When you first start training, the network hasn't learned how important whitespace is, so output will look something like\n",
    "\n",
    "\n",
    "```FollowslammedagainFluffyslightlyfirmlydarkcroakedSurehomeworkapplausefinetransparenthittimidly ```\n",
    "\n",
    "\n",
    "It got better with short phrases again.\n",
    "\n",
    "```Harry muttered at the fire.```\n",
    "\n",
    "```\"The dormitory,\" said Ron.```\n",
    "\n",
    "```\"Ron will do!\"```\n",
    "\n",
    "But it ran into similar problems as the character level RNN\n",
    "\n",
    "```Don't with Dobby whispered it inside their heads.```\n",
    "\n",
    "```\"Don't no?\" said Professor slowly, her smile seemed more\n",
    "great.```\n",
    "\n",
    "```Now, ridiculous,\" The  whispered.```\n",
    "\n",
    "```\"Flint done. Come, it should sometimes have better Dumbledore\"```\n",
    "\n",
    "There were some funny moments where you could see word2vec had produced some similar words, but they weren't synonyms in this\n",
    "case like when instead of ```black hole``` it wrote ```black whole```.\n",
    "\n",
    "Ultimately, the improvement over the character-level RNN was not what I hoped for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2\n",
    "\n",
    "After underwhelming results in regards to coherent, natural text generation, I started researching pretrained models available. [GPT-2](https://openai.com/blog/better-language-models/) emerged as a good option. Although the most powerful version hasn't been released to the general public, I still found impressive results with its weaker models. In addition, I found an excellent open source package on Github called [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) which makes downloading, loading, fine-tuning, and sampling models super easy. \n",
    "\n",
    "The models can be pretty large so I haven't included them in the repo, but it doesn't take long to fine tune them.\n",
    "\n",
    "If you don't have gpt-2-simple, you can get it by executing:\n",
    "```pip install gpt-2-simple```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gpt-2-simple # Uncomment and run this to install\n",
    "import gpt_2_simple as gpt2\n",
    "\n",
    "model_name = \"345M\" # Options are 117M parameter model of 345M parameter model\n",
    "filename = 'hp_all_concat.txt'\n",
    "lr=1e-4\n",
    "iterations = 1000\n",
    "restore_from_checkpoint = 'latest' # latest or path to checkpoint to start training from\n",
    "print_every = 1\n",
    "save_model_every = 1000\n",
    "sample_every = 100 # print a sample of generated text of length sample_length every sample_every iterations\n",
    "sample_length = 1000\n",
    "\n",
    "\n",
    "gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/117M/ or /models/345M/\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              filename,\n",
    "              model_name=model_name,\n",
    "              steps=iterations,\n",
    "              learning_rate=lr,\n",
    "              print_every=print_every,\n",
    "              save_every=save_model_every,\n",
    "              restore_from=restore_from_checkpoint,\n",
    "              sample_every=sample_every,\n",
    "              sample_length=sample_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a chunk of text\n",
    "seed = None\n",
    "top_k = 0\n",
    "top_p = 0\n",
    "temperature = 0.7\n",
    "length = 1000\n",
    "num_samples = 1\n",
    "\n",
    "gpt2.generate(sess,\n",
    "              seed=seed,\n",
    "              top_k=top_k,\n",
    "              top_p=top_p,\n",
    "              temperature=temperature,\n",
    "              length=length,\n",
    "              nsamples=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The finetuned gpt-2 model performed MUCH better and was the only model that could consistently give coherent sentence+ length responses. Early on, you could tell the finetuning hadn't overridden the general knowledge the model came in with and it came up with a lot of semicoherent responses that made no sense in the context of the Harry Potter universe.\n",
    "\n",
    "```\"Sometimes I wish I'd knew what the Wizards were all about.\" Ron's fists were\n",
    "like lead. \"I wish I'd had a list of all the stuff we hate about the Dursleys,\n",
    "that's very topical. For years, I'd been trying to get a copy of Magic: The Gathering\n",
    "with my wife and I.```\n",
    "\n",
    "```\"You think we're going to have to go all the way down to Gringotts to get a\n",
    "fly in here?\" said Uncle Vernon```\n",
    "\n",
    "However it quickly got better.\n",
    "\n",
    "```As far back as Dudley's nightmares had gone, Harry had never been so\n",
    "sad and wrong. ```\n",
    "\n",
    "```She was holding a large pink umbrella, and looking through the\n",
    "walls on her bushy brown head you can see her\n",
    "through the crack in the white curtains.\n",
    "\"How long have I been up?\" she said ```\n",
    "\n",
    "``` \"We're going out now,\" said Ron, tearing his eyes off another boat. \"We're\n",
    "done for.```\n",
    "\n",
    "Still, there were some wacky outputs\n",
    "\n",
    "``` \"Wizard's duel, Round 3,\" said Ron. He pulled out a toad and paced around\n",
    "the boy.\n",
    "\"Oh, are you doing Herbology?\" he asked Ron excitedly.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "gpt-2 seemed to be far and away the best model for text generation and the robust tools available for finetuning make it super quick toget a specialized model up and running. Also, top p sampling/nucleus sampling is a clever idea that avoids the issue with top k sampling on a probability distrubition where there are k+x probable choices(for some x > 0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
